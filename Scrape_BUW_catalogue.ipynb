{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrape_BUW_catalogue.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/Elbereth-Elentari/Book_recommender/blob/master/Scrape_BUW_catalogue.ipynb",
      "authorship_tag": "ABX9TyOxdvGVPFUSSabBLTXupMi4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elbereth-Elentari/Book_recommender/blob/master/Scrape_BUW_catalogue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPPuHJkGhliT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install scrapy\n",
        "!pip3 install bs4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqelvzQssPg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import logging\n",
        "\n",
        "\n",
        "class JsonWriterPipeline(object):\n",
        "\n",
        "    def open_spider(self, spider):\n",
        "        self.file = open('/content/drive/My Drive/Library_catalogue.jl', 'w')\n",
        "\n",
        "    def close_spider(self, spider):\n",
        "        self.file.close()\n",
        "\n",
        "    def process_item(self, item, spider):\n",
        "        line = json.dumps(dict(item)) + \"\\n\"\n",
        "        self.file.write(line)\n",
        "        return item\n",
        "\n",
        "\n",
        "class BookSpider(scrapy.Spider):\n",
        "    name = 'books'\n",
        "    custom_settings = {\n",
        "        'LOG_LEVEL': logging.WARNING,\n",
        "        'LOG_FILE': '/content/drive/My Drive/Scrapy2.log',\n",
        "        'ITEM_PIPELINES': {'__main__.JsonWriterPipeline': 100},\n",
        "    }\n",
        "\n",
        "\n",
        "    def start_requests(self):\n",
        "        #urls = [f'https://chamo.buw.uw.edu.pl/search/query?filter_format=book&filter_lang=pol&filter_lang=eng&filter_loc=10000&filter_loc=10002&sort=dateCreated&pageNumber={page}&theme=system' for page in range(1,74856)]\n",
        "        with open('/content/drive/My Drive/Scrapy.log', 'r') as old_log:\n",
        "            for line in old_log:\n",
        "                if '[scrapy.core.scraper] ERROR: Spider error processing' in line:\n",
        "                    url = re.search('<GET (.+?)>', line).group(1)\n",
        "                    yield scrapy.Request(url=url, callback=self.parse)\n",
        "\n",
        "\n",
        "    def parse(self, response):\n",
        "        soup = BeautifulSoup(response.body, 'html.parser')\n",
        "        records = soup.find(class_='records').find_all('div', class_='record')\n",
        "        \n",
        "        author, storage, WD_signature, publisher,  = '', '', '', ''\n",
        "        pages, year = 0, 0\n",
        "        for record in records:\n",
        "            if 'Adres wyd.' in record.text and 'Opis fiz.' in record.text:\n",
        "                title = record.find(class_='title').text\n",
        "                title = re.sub(r' [=/].+', '', title)\n",
        "                try: author = record.find(class_='author').text\n",
        "                except: pass\n",
        "                if 'BUW Magazyn' in record.text: storage = 'magazyn'\n",
        "\n",
        "                infos = record.find_all('tr')\n",
        "                for info in infos:\n",
        "                    if 'Klasyfikacja WD' in info.text:\n",
        "                        WD_signature = info.a.text\n",
        "                    elif 'Adres wyd.' in info.text:\n",
        "                        publisher_candidates = info.find_all('span')\n",
        "                        for publisher in publisher_candidates:\n",
        "                            if ('class' in publisher and publisher['class'] != 'highlight') or 'class' not in publisher:\n",
        "                                publisher_with_colon = re.search(r\".+ : ?([\\w .']+),?.*(\\d{4})\", publisher.text)\n",
        "                                if publisher_with_colon:\n",
        "                                    publisher = publisher_with_colon.group(1)\n",
        "                                    year = int(publisher_with_colon.group(2))\n",
        "                                else:\n",
        "                                    publisher = re.search(r\".+? ([\\w .']+),?.*(\\d{4})\", publisher.text)\n",
        "                                    if publisher:\n",
        "                                        year = int(publisher.group(2))\n",
        "                                        publisher = publisher.group(1)\n",
        "                    elif 'Opis fiz.' in info.text:\n",
        "                        pages = info.find('span').text\n",
        "                        pages = re.sub(r'\\[.+?\\]', '', pages)\n",
        "                        pages = int(re.search(r'\\d+', pages).group(0))\n",
        "\n",
        "                yield {'title': title,\n",
        "                       'author': author,\n",
        "                       'storage': storage,\n",
        "                       'WD_signature': WD_signature,\n",
        "                       'publisher': publisher,\n",
        "                       'year': year,\n",
        "                       'pages': pages}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD7ibjI9hnYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy startproject buw\n",
        "!cd buw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID89T8dqkD5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings\n",
        "\n",
        "process = CrawlerProcess(get_project_settings())\n",
        "\n",
        "process.crawl(BookSpider)\n",
        "process.start()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}